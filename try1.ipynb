{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a6b234",
   "metadata": {},
   "source": [
    "A nice pattern could be to define a function that accepts the target year and a filepath as an argument and then checks at the specified filepath for a file corresponding to the dataset you are going to download. If it exists, it could immediately open up the file and return the saved contents. If not, it could download the contents from the REST API and save it to the file before returning the data. This means that the first time you run this function, it will automatically download and cache the data locally, and the next time you run the same function, it will instead load the local data. Consider using environment variables to allow each teammate to specify different locations, and having your function automatically retrieve the location specified by the environment variable so you don’t have to fight about paths in your git repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97318f66",
   "metadata": {},
   "source": [
    "seasons: 2016,2017,2018,2019,2020\n",
    "02 = regular season, 03 = playoffs,\n",
    "\n",
    "The first 4 digits identify the season of the game (ie. 2017 for the 2017-2018 season). The next 2 digits give the type of game, where 01 = preseason, 02 = regular season, 03 = playoffs, 04 = all-star. The final 4 digits identify the specific game number. For regular season and preseason games, this ranges from 0001 to the number of games played. (1271 for seasons with 31 teams (2017 and onwards) and 1230 for seasons with 30 teams). For playoff games, the 2nd digit of the specific number gives the round of the playoffs, the 3rd digit specifies the matchup, and the 4th digit specifies the game (out of 7).\n",
    "\n",
    "regular:\n",
    "combinations formed for 2016: year/02/0001to1230 \n",
    "combinations formed for 2017se2020: year/02/0001to1271\n",
    "\n",
    "playoff:\n",
    "combinations formed for: year/03/four digits\n",
    "digit1=0\n",
    "digit2=1/2/3/4\n",
    "digit3=1-6/1-4/1-2/1\n",
    "digit4=1/2/3/4/5/6/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b844ddc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/nwzkvw4x5g55cyxllzlc0gkm0000gn/T/ipykernel_1053/2585230290.py:49: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n",
      "  response_p = urlopen(url_p,cafile=certifi.where())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 2016030111.csv\n",
      "200\n",
      "1 1 2 2016030112.csv\n",
      "200\n",
      "1 1 3 2016030113.csv\n",
      "200\n",
      "1 1 4 2016030114.csv\n",
      "200\n",
      "1 1 5 2016030115.csv\n",
      "200\n",
      "1 1 6 2016030116.csv\n",
      "200\n",
      "1 1 7 2016030117.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "1 2 1 2016030121.csv\n",
      "200\n",
      "1 2 2 2016030122.csv\n",
      "200\n",
      "1 2 3 2016030123.csv\n",
      "200\n",
      "1 2 4 2016030124.csv\n",
      "200\n",
      "1 2 5 2016030125.csv\n",
      "200\n",
      "1 2 6 2016030126.csv\n",
      "200\n",
      "1 2 7 2016030127.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "1 3 1 2016030131.csv\n",
      "200\n",
      "1 3 2 2016030132.csv\n",
      "200\n",
      "1 3 3 2016030133.csv\n",
      "200\n",
      "1 3 4 2016030134.csv\n",
      "200\n",
      "1 3 5 2016030135.csv\n",
      "200\n",
      "1 3 6 2016030136.csv\n",
      "200\n",
      "1 3 7 2016030137.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "1 4 1 2016030141.csv\n",
      "200\n",
      "1 4 2 2016030142.csv\n",
      "200\n",
      "1 4 3 2016030143.csv\n",
      "200\n",
      "1 4 4 2016030144.csv\n",
      "200\n",
      "1 4 5 2016030145.csv\n",
      "200\n",
      "1 4 6 2016030146.csv\n",
      "404\n",
      "404\n",
      "404\n",
      "200\n",
      "1 5 1 2016030151.csv\n",
      "200\n",
      "1 5 2 2016030152.csv\n",
      "200\n",
      "1 5 3 2016030153.csv\n",
      "200\n",
      "1 5 4 2016030154.csv\n",
      "200\n",
      "1 5 5 2016030155.csv\n",
      "200\n",
      "1 5 6 2016030156.csv\n",
      "200\n",
      "1 5 7 2016030157.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "1 6 1 2016030161.csv\n",
      "200\n",
      "1 6 2 2016030162.csv\n",
      "200\n",
      "1 6 3 2016030163.csv\n",
      "200\n",
      "1 6 4 2016030164.csv\n",
      "200\n",
      "1 6 5 2016030165.csv\n",
      "200\n",
      "1 6 6 2016030166.csv\n",
      "200\n",
      "1 6 7 2016030167.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "1 7 1 2016030171.csv\n",
      "200\n",
      "1 7 2 2016030172.csv\n",
      "200\n",
      "1 7 3 2016030173.csv\n",
      "200\n",
      "1 7 4 2016030174.csv\n",
      "200\n",
      "1 7 5 2016030175.csv\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "200\n",
      "1 8 1 2016030181.csv\n",
      "200\n",
      "1 8 2 2016030182.csv\n",
      "200\n",
      "1 8 3 2016030183.csv\n",
      "200\n",
      "1 8 4 2016030184.csv\n",
      "200\n",
      "1 8 5 2016030185.csv\n",
      "200\n",
      "1 8 6 2016030186.csv\n",
      "200\n",
      "1 8 7 2016030187.csv\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "200\n",
      "2 1 1 2016030211.csv\n",
      "200\n",
      "2 1 2 2016030212.csv\n",
      "200\n",
      "2 1 3 2016030213.csv\n",
      "200\n",
      "2 1 4 2016030214.csv\n",
      "200\n",
      "2 1 5 2016030215.csv\n",
      "200\n",
      "2 1 6 2016030216.csv\n",
      "200\n",
      "2 1 7 2016030217.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "2 2 1 2016030221.csv\n",
      "200\n",
      "2 2 2 2016030222.csv\n",
      "200\n",
      "2 2 3 2016030223.csv\n",
      "200\n",
      "2 2 4 2016030224.csv\n",
      "200\n",
      "2 2 5 2016030225.csv\n",
      "200\n",
      "2 2 6 2016030226.csv\n",
      "200\n",
      "2 2 7 2016030227.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "2 3 1 2016030231.csv\n",
      "200\n",
      "2 3 2 2016030232.csv\n",
      "200\n",
      "2 3 3 2016030233.csv\n",
      "200\n",
      "2 3 4 2016030234.csv\n",
      "200\n",
      "2 3 5 2016030235.csv\n",
      "200\n",
      "2 3 6 2016030236.csv\n",
      "200\n",
      "2 3 7 2016030237.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "2 4 1 2016030241.csv\n",
      "200\n",
      "2 4 2 2016030242.csv\n",
      "200\n",
      "2 4 3 2016030243.csv\n",
      "200\n",
      "2 4 4 2016030244.csv\n",
      "200\n",
      "2 4 5 2016030245.csv\n",
      "200\n",
      "2 4 6 2016030246.csv\n",
      "200\n",
      "2 4 7 2016030247.csv\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "200\n",
      "3 1 1 2016030311.csv\n",
      "200\n",
      "3 1 2 2016030312.csv\n",
      "200\n",
      "3 1 3 2016030313.csv\n",
      "200\n",
      "3 1 4 2016030314.csv\n",
      "200\n",
      "3 1 5 2016030315.csv\n",
      "200\n",
      "3 1 6 2016030316.csv\n",
      "200\n",
      "3 1 7 2016030317.csv\n",
      "404\n",
      "404\n",
      "200\n",
      "3 2 1 2016030321.csv\n",
      "200\n",
      "3 2 2 2016030322.csv\n",
      "200\n",
      "3 2 3 2016030323.csv\n",
      "200\n",
      "3 2 4 2016030324.csv\n",
      "200\n",
      "3 2 5 2016030325.csv\n",
      "200\n",
      "3 2 6 2016030326.csv\n",
      "200\n",
      "3 2 7 2016030327.csv\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "200\n",
      "4 1 1 2016030411.csv\n",
      "200\n",
      "4 1 2 2016030412.csv\n",
      "200\n",
      "4 1 3 2016030413.csv\n",
      "200\n",
      "4 1 4 2016030414.csv\n",
      "200\n",
      "4 1 5 2016030415.csv\n",
      "200\n",
      "4 1 6 2016030416.csv\n",
      "200\n",
      "4 1 7 2016030417.csv\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "'''from urllib.request import urlopen\n",
    "import json\n",
    "import certifi\n",
    "import pandas as pd\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def extract(year):\n",
    "    \n",
    "    playoff_code=\"0\"+str(3);\n",
    "    \n",
    "\n",
    "#regular data file\n",
    "    regular_code=\"0\"+str(2);\n",
    "    a=[\"%04d\" % x for x in range(1230)]\n",
    "    b=[\"%04d\" % x for x in range(1271)]\n",
    "    \n",
    "    if(year==2016):\n",
    "        for i in range(1,1231):\n",
    "            code_r=str(a[i])\n",
    "            url_r = \"https://statsapi.web.nhl.com/api/v1/game/\"+str(year)+regular_code+code_r+\"/feed/live/\"\n",
    "            response_r = urlopen(url_r,cafile=certifi.where())\n",
    "            data_r = json.loads(response_r.read())\n",
    "            df_r = pd.DataFrame.from_dict(data_r)\n",
    "            file_name_r=str(year)+regular_code+code_r+\".csv\"\n",
    "            df_r.to_csv (file_name_r, index = False, header=True)\n",
    "\n",
    "    else:\n",
    "        for i in range(1,1271):\n",
    "            code_r=str(b[i])\n",
    "            url_r = \"https://statsapi.web.nhl.com/api/v1/game/\"+str(year)+regular_code+code_r+\"/feed/live/\"\n",
    "            response_r = urlopen(url_r,cafile=certifi.where())\n",
    "            data_r = json.loads(response_r.read())\n",
    "            df_r = pd.DataFrame.from_dict(data_r)\n",
    "            file_name_r=str(year)+regular_code+code_r+\".csv\"\n",
    "            df_r.to_csv (file_name_r, index = False, header=True)\n",
    "        \n",
    "\n",
    "#playoff data file (in progress code)\n",
    "    \n",
    "    for digit2 in range(1,5):\n",
    "        for digit3 in range(1,8):\n",
    "            for digit4 in range(1,8):\n",
    "                code_p=\"0\"+str(digit2)+str(digit3)+str(digit4)\n",
    "                #code_p=\"0256\"\n",
    "                url_p = \"https://statsapi.web.nhl.com/api/v1/game/\"+str(year)+playoff_code+code_p+\"/feed/live/\"\n",
    "                remove_url = requests.get(url_p)\n",
    "                #print(remove_url.status_code)\n",
    "                if (remove_url.status_code!=404):\n",
    "                    response_p = urlopen(url_p,cafile=certifi.where())\n",
    "                    #print(response_p.getcode())\n",
    " \n",
    "                    data_p = json.loads(response_p.read())\n",
    "                        \n",
    "                    df_p = pd.DataFrame.from_dict(data_p) \n",
    "                    file_name_p=str(year)+playoff_code+code_p+\".csv\"\n",
    "                    #print(digit2,digit3,digit4,file_name_p)\n",
    "                    df_p.to_csv (file_name_p, index = False, header=True)\n",
    "                \n",
    "    \n",
    "extract(2016) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8318e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import json\n",
    "import certifi\n",
    "import pandas as pd\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def extract(year):\n",
    "    \n",
    "    playoff_code=\"0\"+str(3);\n",
    "    \n",
    "\n",
    "#regular data file\n",
    "    regular_code=\"0\"+str(2);\n",
    "    a=[\"%04d\" % x for x in range(1230)]\n",
    "    b=[\"%04d\" % x for x in range(1271)]\n",
    "    \n",
    "    if(year==2016):\n",
    "        for i in range(1,1230):\n",
    "            code_r=str(a[i])\n",
    "            url_r = \"https://statsapi.web.nhl.com/api/v1/game/\"+str(year)+regular_code+code_r+\"/feed/live/\"\n",
    "            response_r = urlopen(url_r,cafile=certifi.where())\n",
    "            data_r = json.loads(response_r.read())\n",
    "            df_r = pd.DataFrame.from_dict(data_r)\n",
    "            file_name_r=str(year)+regular_code+code_r+\".json\"\n",
    "            json_string = json.dumps(data_r)\n",
    "            with open(file_name_r, 'w') as outfile:\n",
    "              json.dump(json_string, outfile)\n",
    "            #df_r.to_csv (file_name_r, index = False, header=True)\n",
    "\n",
    "    else:\n",
    "        for i in range(1,1271):\n",
    "            code_r=str(b[i])\n",
    "            url_r = \"https://statsapi.web.nhl.com/api/v1/game/\"+str(year)+regular_code+code_r+\"/feed/live/\"\n",
    "            response_r = urlopen(url_r,cafile=certifi.where())\n",
    "            data_r = json.loads(response_r.read())\n",
    "            df_r = pd.DataFrame.from_dict(data_r)\n",
    "            file_name_r=str(year)+regular_code+code_r+\".json\"\n",
    "            json_string = json.dumps(data_r)\n",
    "            with open(file_name_r, 'w') as outfile:\n",
    "              json.dump(json_string, outfile)\n",
    "            #df_r.to_csv (file_name_r, index = False, header=True)\n",
    "        \n",
    "\n",
    "#playoff data file (in progress code)\n",
    "    \n",
    "    for digit2 in range(1,5):\n",
    "        for digit3 in range(1,8):\n",
    "            for digit4 in range(1,8):\n",
    "                code_p=\"0\"+str(digit2)+str(digit3)+str(digit4)\n",
    "                #code_p=\"0256\"\n",
    "                url_p = \"https://statsapi.web.nhl.com/api/v1/game/\"+str(year)+playoff_code+code_p+\"/feed/live/\"\n",
    "                remove_url = requests.get(url_p)\n",
    "                #print(remove_url.status_code)\n",
    "                if (remove_url.status_code!=404):\n",
    "                    response_p = urlopen(url_p,cafile=certifi.where())\n",
    "                    #print(response_p.getcode())\n",
    " \n",
    "                    data_p = json.loads(response_p.read())\n",
    "                        \n",
    "                    df_p = pd.DataFrame.from_dict(data_p) \n",
    "                    file_name_p=str(year)+playoff_code+code_p+\".json\"\n",
    "                    json_string = json.dumps(data_p)\n",
    "                    with open(file_name_p, 'w') as outfile:\n",
    "                      json.dump(json_string, outfile)\n",
    "                    #print(digit2,digit3,digit4,file_name_p)\n",
    "                   # df_p.to_csv (file_name_p, index = False, header=True)\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f9347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e09b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url=\"https://statsapi.web.nhl.com/api/v1/game/2016030256/feed/live/\"\n",
    "remove_url = requests.get(url)\n",
    "print(remove_url.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c853845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7870e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97037220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "def tidy(year) -> pd.DataFrame:\n",
    "\n",
    "  \"\"\"\n",
    "  Clean the json files downloaded with get_data.py function\n",
    "  df : pd.DataFrame\n",
    "  Returns\n",
    "  pd.DataFrame\n",
    "      pandas DataFrame of the play-by-play data where each row is an play event.\n",
    "      with column names:\n",
    "          events_types: events of the type “shots” and “goals”, missed shots or blocked shots for now.\n",
    "          DONE game_time: game time/period information\n",
    "          DONE game_id: game ID\n",
    "          DONE team_info: team information (which team took the shot)\n",
    "          DONEis_shot: indicator if its a shot or a goal\n",
    "          DONEcoordinates_x, coordinates_y: the on-ice coordinates\n",
    "          DONEshooter_name, goalie_name: the shooter and goalie name (don’t worry about assists for now)\n",
    "          DONEshot_type: shot type\n",
    "          **DONEempty_name: if it was on an empty net\n",
    "          DONEstrength:  whether or not a goal was at even strength, shorthanded, or on the power play.\n",
    "  \"\"\"\n",
    "  event_idx, period_time, period, game_id, team_away_name, team_home_name, is_goal, coordinate_x, coordinate_y, shot_type, strength, shooter_name, goalie_name, empty_net, team_name = ([] for i in range(15))\n",
    "  df_main=pd.DataFrame()\n",
    "  files_for_year = [filename for filename in os.listdir() if filename.startswith(str(year))]\n",
    "  for f in files_for_year:\n",
    "      with open(f, encoding='utf-8', errors='ignore') as json_file:\n",
    "        data = json.load(json_file, strict=False)\n",
    "        data_processed=json.loads(data)\n",
    "        allplays_data = data_processed['liveData']['plays']['allPlays']\n",
    "        gameID = f.split('.', 1)[0]\n",
    "        for j in range(len(allplays_data)):\n",
    "          if(allplays_data[j]['result']['eventTypeId'] == (\"SHOT\" or \"GOAL\")):\n",
    "            period.append(allplays_data[j]['about']['period'])\n",
    "            period_time.append(allplays_data[j]['about']['periodTime'])\n",
    "            game_id.append(gameID)\n",
    "            event_idx.append(allplays_data[j]['about']['eventIdx'])\n",
    "            #team_away_name.append(df.iloc[:,i]['gameData']['teams']['away']['name'])\n",
    "            #team_away_name.append('demovalue')\n",
    "            #team_home_name.append('demovalue')\n",
    "            #team_home_name.append(df.iloc[:,i]['gameData']['teams']['home']['name'])\n",
    "            team_name.append(allplays_data[j]['team']['name'])\n",
    "            is_goal.append(allplays_data[j]['result']['eventTypeId']==\"GOAL\")\n",
    "            coordinate_x.append(allplays_data[j]['coordinates']['x'] if  'x' in allplays_data[j]['coordinates'] else np.nan)\n",
    "            coordinate_y.append(allplays_data[j]['coordinates']['y'] if  'y' in allplays_data[j]['coordinates'] else np.nan)\n",
    "            shot_type.append(allplays_data[j]['result']['secondaryType'] if 'secondaryType' in allplays_data[j]['result'] else np.nan)\n",
    "            strength.append(allplays_data[j]['result']['strength']['code'] if 'strength' in allplays_data[j]['result'] else np.nan)\n",
    "            if (allplays_data[j]['players'][z]['playerType'] == (\"Shooter\" or 'Scorer') for z in range(len(allplays_data[j]['players']))):\n",
    "              shooter_name.append([allplays_data[j]['players'][z]['player']['fullName'] for z in range(len(allplays_data[j]['players']))][0])\n",
    "            if (allplays_data[j]['players'][z]['playerType']==\"Goalie\" for z in range(len(allplays_data[j]['players']))):\n",
    "              goalie_name.append([allplays_data[j]['players'][z]['player']['fullName'] for z in range(len(allplays_data[j]['players']))][0])\n",
    "              empty_net.append(True if 'emptyNet' in allplays_data[j]['result'] and allplays_data[j]['result']['emptyNet']==True else False)\n",
    "\n",
    "  assert(all(len(lists) == len(game_id) for lists in [event_idx, period_time, period, is_goal, coordinate_x,\\\n",
    "          coordinate_y, shot_type, strength, shooter_name, goalie_name, empty_net, team_name]) )\n",
    "\n",
    "  df_main1 = pd.DataFrame(np.column_stack([event_idx, period_time, period, game_id, is_goal, coordinate_x,\\\n",
    "          coordinate_y, shot_type, strength, shooter_name, goalie_name, empty_net, team_name]),\n",
    "                            columns=['event_idx', 'period_time', 'period', 'game_id', 'is_goal', 'coordinate_x',\n",
    "                            'coordinate_y', 'shot_type', 'strength', 'shooter_name','goalie_name', 'empty_net', 'team_name'])\n",
    "  df_main=df_main.append(df_main1)\n",
    "      \n",
    "  return df_main\n",
    "\n",
    "def tidyAll():\n",
    "  complete_data=pd.DataFrame()\n",
    "  for i in range(2016,2021):\n",
    "    complete_data = complete_data.append(tidy(i))\n",
    "  return complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9cb472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [event_idx, period_time, period, game_id, is_goal, coordinate_x, coordinate_y, shot_type, strength, shooter_name, goalie_name, empty_net, team_name]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_main=df_main.append(df_main1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  complete_data = complete_data.append(tidy(i))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_main=df_main.append(df_main1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  complete_data = complete_data.append(tidy(i))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_main=df_main.append(df_main1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  complete_data = complete_data.append(tidy(i))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_main=df_main.append(df_main1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  complete_data = complete_data.append(tidy(i))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_main=df_main.append(df_main1)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:71: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  complete_data = complete_data.append(tidy(i))\n"
     ]
    }
   ],
   "source": [
    "df_final = tidyAll()\n",
    "print(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f9ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [event_idx, period_time, period, game_id, is_goal, coordinate_x, coordinate_y, shot_type, strength, shooter_name, goalie_name, empty_net, team_name]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14156\\1631773846.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_main=df_main.append(df_main1)\n"
     ]
    }
   ],
   "source": [
    "df_new=tidy(2016)\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba83b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
